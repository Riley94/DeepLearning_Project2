{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hASLpyncBmvt"
      },
      "outputs": [],
      "source": [
        "!pip install -U portalocker>=2.0.0 optuna-integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jum_uBK3SA8Q"
      },
      "source": [
        "## Task - 1\n",
        "\n",
        "### PyTorch FC ANN MNIST Implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1AShoC94SHM6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from optuna.integration import PyTorchLightningPruningCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YDg8ZzyzSKpu"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "train_val_split = 0.8\n",
        "\n",
        "# This dataset is already \"sorted\" as part of the import method, but no \"validation\" set has been selected in this case\n",
        "# Loading the FashionMNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Calculate sizes\n",
        "train_size = int(train_val_split * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3DRu0tGPS0dy"
      },
      "outputs": [],
      "source": [
        "# Mapping the labels for the MNIST dataset -- later we'll see that this using the \"keras to_categorical\" method as discussed in class\n",
        "labels_map = {\n",
        "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
        "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_z_Aj5LWS_-f"
      },
      "outputs": [],
      "source": [
        "# Here we define the model parameters -- the general strucutre as provided here will produce a fully connected network [28x28] --> 32 --> 16 --> 10\n",
        "class MLP(nn.Module): # MLP stands for \"Multi-Layer Perceptron\"\n",
        "    def __init__(self, layer_sizes, activation_functions): # this initializes the structure of the network\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        input_size = 28 * 28 ## 28*28 input features and 32 outputs\n",
        "        for size in layer_sizes:\n",
        "            self.layers.append(nn.Linear(input_size, size))\n",
        "            input_size = size\n",
        "        \n",
        "        self.activation_functions = activation_functions\n",
        "        self.output_layer = nn.Linear(input_size, 10) ## 10 output features because MNIST has 10 target classes\n",
        "\n",
        "    def forward(self, x): # this modifies the elements of the intial structure defined above\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        for layer, activation in zip(self.layers, self.activation_functions):\n",
        "            x = activation(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-10-17 09:19:17,813] A new study created in memory with name: no-name-7a51d881-bb0d-4b97-8581-83e3fb79fc14\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_4412\\2204145081.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_4412\\2204145081.py:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_4412\\2204145081.py:46: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  momentum = trial.suggest_uniform('momentum', 0.0, 0.9)\n",
            "[I 2024-10-17 09:20:13,131] Trial 0 finished with value: 0.11066666666666666 and parameters: {'num_layers': 3, 'layer_size_0': 256, 'activation_0': 'sigmoid', 'layer_size_1': 32, 'activation_1': 'tanh', 'layer_size_2': 128, 'activation_2': 'sigmoid', 'optimizer': 'SGD', 'lr': 0.0009005173656850634, 'weight_decay': 0.0035490759708404415, 'batch_size': 64, 'momentum': 0.28149339780212806}. Best is trial 0 with value: 0.11066666666666666.\n",
            "[I 2024-10-17 09:21:05,216] Trial 1 finished with value: 0.7723333333333333 and parameters: {'num_layers': 2, 'layer_size_0': 64, 'activation_0': 'tanh', 'layer_size_1': 32, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.010291670142615013, 'weight_decay': 0.0002621413018002615, 'batch_size': 128}. Best is trial 1 with value: 0.7723333333333333.\n",
            "[I 2024-10-17 09:21:59,434] Trial 2 finished with value: 0.902 and parameters: {'num_layers': 4, 'layer_size_0': 64, 'activation_0': 'sigmoid', 'layer_size_1': 64, 'activation_1': 'sigmoid', 'layer_size_2': 128, 'activation_2': 'tanh', 'layer_size_3': 256, 'activation_3': 'tanh', 'optimizer': 'SGD', 'lr': 0.08829140126591851, 'weight_decay': 1.2941596394441833e-06, 'batch_size': 128, 'momentum': 0.5865312873544405}. Best is trial 2 with value: 0.902.\n",
            "[I 2024-10-17 09:22:53,624] Trial 3 finished with value: 0.1115 and parameters: {'num_layers': 3, 'layer_size_0': 64, 'activation_0': 'relu', 'layer_size_1': 64, 'activation_1': 'sigmoid', 'layer_size_2': 64, 'activation_2': 'relu', 'optimizer': 'SGD', 'lr': 2.0432690731453507e-05, 'weight_decay': 0.0003004308133168588, 'batch_size': 64, 'momentum': 0.16194094305595708}. Best is trial 2 with value: 0.902.\n",
            "[I 2024-10-17 09:23:52,450] Trial 4 finished with value: 0.8425833333333334 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'relu', 'layer_size_1': 128, 'activation_1': 'tanh', 'optimizer': 'Adam', 'lr': 2.0083535216606207e-05, 'weight_decay': 0.0007194243511620005, 'batch_size': 32}. Best is trial 2 with value: 0.902.\n",
            "[I 2024-10-17 09:24:47,634] Trial 5 finished with value: 0.8250833333333333 and parameters: {'num_layers': 4, 'layer_size_0': 32, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'sigmoid', 'layer_size_2': 64, 'activation_2': 'sigmoid', 'layer_size_3': 64, 'activation_3': 'tanh', 'optimizer': 'RMSprop', 'lr': 0.00020457057259589198, 'weight_decay': 1.652487004465208e-06, 'batch_size': 64}. Best is trial 2 with value: 0.902.\n",
            "[I 2024-10-17 09:25:40,310] Trial 6 finished with value: 0.8633333333333333 and parameters: {'num_layers': 2, 'layer_size_0': 64, 'activation_0': 'tanh', 'layer_size_1': 32, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 0.004769202908370359, 'weight_decay': 9.224860119922062e-05, 'batch_size': 128, 'momentum': 0.6127814041442424}. Best is trial 2 with value: 0.902.\n",
            "[I 2024-10-17 09:26:39,901] Trial 7 finished with value: 0.9455833333333333 and parameters: {'num_layers': 2, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'optimizer': 'SGD', 'lr': 0.017687904254669623, 'weight_decay': 0.001304789704792876, 'batch_size': 32, 'momentum': 0.1894720749043393}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:27:35,377] Trial 8 finished with value: 0.845 and parameters: {'num_layers': 3, 'layer_size_0': 64, 'activation_0': 'sigmoid', 'layer_size_1': 64, 'activation_1': 'tanh', 'layer_size_2': 128, 'activation_2': 'tanh', 'optimizer': 'Adam', 'lr': 4.783521504935869e-05, 'weight_decay': 1.048819615131936e-05, 'batch_size': 64}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:28:29,181] Trial 9 finished with value: 0.4514166666666667 and parameters: {'num_layers': 2, 'layer_size_0': 64, 'activation_0': 'sigmoid', 'layer_size_1': 256, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 1.6106666878271722e-05, 'weight_decay': 3.561184930084668e-05, 'batch_size': 128}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:29:28,458] Trial 10 finished with value: 0.8828333333333334 and parameters: {'num_layers': 2, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 0.08905087848969276, 'weight_decay': 0.009139670835826081, 'batch_size': 32, 'momentum': 0.037935586835289525}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:30:28,887] Trial 11 finished with value: 0.9226666666666666 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'sigmoid', 'layer_size_1': 64, 'activation_1': 'tanh', 'layer_size_2': 256, 'activation_2': 'tanh', 'layer_size_3': 256, 'activation_3': 'sigmoid', 'optimizer': 'SGD', 'lr': 0.06118995738329661, 'weight_decay': 2.3446560329036185e-06, 'batch_size': 32, 'momentum': 0.8474293365104729}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:31:29,342] Trial 12 finished with value: 0.9390833333333334 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'sigmoid', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 256, 'activation_2': 'tanh', 'layer_size_3': 128, 'activation_3': 'sigmoid', 'optimizer': 'SGD', 'lr': 0.01729772077899312, 'weight_decay': 1.1053846308379612e-05, 'batch_size': 32, 'momentum': 0.8323149298805409}. Best is trial 7 with value: 0.9455833333333333.\n",
            "[I 2024-10-17 09:32:27,556] Trial 13 finished with value: 0.9475833333333333 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'layer_size_3': 128, 'activation_3': 'relu', 'optimizer': 'SGD', 'lr': 0.006808817137082687, 'weight_decay': 1.4154590111434038e-05, 'batch_size': 32, 'momentum': 0.8827750523526923}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:33:26,510] Trial 14 finished with value: 0.8574166666666667 and parameters: {'num_layers': 3, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'optimizer': 'SGD', 'lr': 0.0020556172624704924, 'weight_decay': 0.0014308302822307164, 'batch_size': 32, 'momentum': 0.35468344092797566}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:34:31,872] Trial 15 finished with value: 0.11066666666666666 and parameters: {'num_layers': 3, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'optimizer': 'Adam', 'lr': 0.018118984478580697, 'weight_decay': 2.8676759490958935e-05, 'batch_size': 32}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:35:30,241] Trial 16 finished with value: 0.41325 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 128, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'layer_size_3': 32, 'activation_3': 'relu', 'optimizer': 'SGD', 'lr': 0.0006420930235733002, 'weight_decay': 7.636912064429874e-06, 'batch_size': 32, 'momentum': 0.4865956081896017}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:36:30,055] Trial 17 finished with value: 0.8686666666666667 and parameters: {'num_layers': 3, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'relu', 'layer_size_2': 32, 'activation_2': 'relu', 'optimizer': 'SGD', 'lr': 0.003717603426842377, 'weight_decay': 0.00010631185001773124, 'batch_size': 32, 'momentum': 0.0025211508259636306}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:37:35,501] Trial 18 finished with value: 0.10208333333333333 and parameters: {'num_layers': 2, 'layer_size_0': 256, 'activation_0': 'tanh', 'layer_size_1': 256, 'activation_1': 'tanh', 'optimizer': 'Adam', 'lr': 0.02678251491099669, 'weight_decay': 0.002018691305592685, 'batch_size': 32}. Best is trial 13 with value: 0.9475833333333333.\n",
            "[I 2024-10-17 09:38:37,295] Trial 19 finished with value: 0.9339166666666666 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'sigmoid', 'layer_size_3': 128, 'activation_3': 'relu', 'optimizer': 'RMSprop', 'lr': 0.00026698072789236297, 'weight_decay': 0.00048051926503555334, 'batch_size': 32}. Best is trial 13 with value: 0.9475833333333333.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'layer_size_3': 128, 'activation_3': 'relu', 'optimizer': 'SGD', 'lr': 0.006808817137082687, 'weight_decay': 1.4154590111434038e-05, 'batch_size': 32, 'momentum': 0.8827750523526923}\n"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 4)\n",
        "    layer_sizes = []\n",
        "    activation_functions = []\n",
        "    activations = {\n",
        "        'relu': nn.ReLU(),\n",
        "        'sigmoid': nn.Sigmoid(),\n",
        "        'tanh': nn.Tanh(),\n",
        "        # Add more if needed\n",
        "    }\n",
        "\n",
        "    optimizers = {\n",
        "        'SGD': optim.SGD,\n",
        "        'Adam': optim.Adam,\n",
        "        'RMSprop': optim.RMSprop,\n",
        "        # Add more optimizers if desired\n",
        "    }\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        layer_size = trial.suggest_categorical(f'layer_size_{i}', [32, 64, 128, 256])\n",
        "        activation_name = trial.suggest_categorical(f'activation_{i}', ['relu', 'sigmoid', 'tanh'])\n",
        "        layer_sizes.append(layer_size)\n",
        "        activation_functions.append(activations[activation_name])\n",
        "\n",
        "    # Suggest optimizer\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'RMSprop'])\n",
        "\n",
        "    # Suggest learning rate\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    # Suggest weight decay (L2 regularization)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
        "\n",
        "    # Suggest batch size\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "\n",
        "    # Initialize the model\n",
        "    model = MLP(layer_sizes, activation_functions)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer_class = optimizers[optimizer_name]\n",
        "    if optimizer_name == 'SGD':\n",
        "        # Suggest momentum for SGD\n",
        "        momentum = trial.suggest_uniform('momentum', 0.0, 0.9)\n",
        "        optimizer = optimizer_class(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Data loaders with suggested batch size\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5  # Adjust as needed\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Best hyperparameters:', study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfpEh_TyTLBa",
        "outputId": "bd6acb53-37dd-446b-a37b-3e856476f51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.1516267788410186\n",
            "Epoch 1, Batch 200, Loss: 1.6584284210205078\n",
            "Epoch 1, Batch 300, Loss: 1.145043471455574\n",
            "Epoch 1, Batch 400, Loss: 0.9123734623193741\n",
            "Epoch 1, Batch 500, Loss: 0.7727144131064415\n",
            "Epoch 1, Batch 600, Loss: 0.6891837051510811\n",
            "Epoch 1, Batch 700, Loss: 0.5949320143461228\n",
            "Epoch 1, Batch 800, Loss: 0.5923074644804001\n",
            "Epoch 1, Batch 900, Loss: 0.55057936668396\n",
            "Epoch 1, Batch 1000, Loss: 0.5502334982156754\n",
            "Epoch 1, Batch 1100, Loss: 0.5631349927186966\n",
            "Epoch 1, Batch 1200, Loss: 0.5101337702572346\n",
            "Epoch 1, Batch 1300, Loss: 0.5203005151450634\n",
            "Epoch 1, Batch 1400, Loss: 0.4783757564425468\n",
            "Epoch 1, Batch 1500, Loss: 0.4745977507531643\n",
            "Validation Loss: 0.4789267548720042\n",
            "Epoch 2, Batch 100, Loss: 0.4644384044408798\n",
            "Epoch 2, Batch 200, Loss: 0.4625648429989815\n",
            "Epoch 2, Batch 300, Loss: 0.45634143337607386\n",
            "Epoch 2, Batch 400, Loss: 0.406360377073288\n",
            "Epoch 2, Batch 500, Loss: 0.4395082722604275\n",
            "Epoch 2, Batch 600, Loss: 0.43375724986195563\n",
            "Epoch 2, Batch 700, Loss: 0.44243573397397995\n",
            "Epoch 2, Batch 800, Loss: 0.404878421574831\n",
            "Epoch 2, Batch 900, Loss: 0.4023264349997044\n",
            "Epoch 2, Batch 1000, Loss: 0.368463700786233\n",
            "Epoch 2, Batch 1100, Loss: 0.3815112745016813\n",
            "Epoch 2, Batch 1200, Loss: 0.3590119432657957\n",
            "Epoch 2, Batch 1300, Loss: 0.3907827547937632\n",
            "Epoch 2, Batch 1400, Loss: 0.34067226462066175\n",
            "Epoch 2, Batch 1500, Loss: 0.333623112142086\n",
            "Validation Loss: 0.34417014944553376\n",
            "Epoch 3, Batch 100, Loss: 0.34424615614116194\n",
            "Epoch 3, Batch 200, Loss: 0.3239162074029446\n",
            "Epoch 3, Batch 300, Loss: 0.3304824049025774\n",
            "Epoch 3, Batch 400, Loss: 0.3461636471003294\n",
            "Epoch 3, Batch 500, Loss: 0.3256107799336314\n",
            "Epoch 3, Batch 600, Loss: 0.32005213990807535\n",
            "Epoch 3, Batch 700, Loss: 0.3138699522987008\n",
            "Epoch 3, Batch 800, Loss: 0.3221431442350149\n",
            "Epoch 3, Batch 900, Loss: 0.31157147787511347\n",
            "Epoch 3, Batch 1000, Loss: 0.32012188725173474\n",
            "Epoch 3, Batch 1100, Loss: 0.3106073825806379\n",
            "Epoch 3, Batch 1200, Loss: 0.3167626906186342\n",
            "Epoch 3, Batch 1300, Loss: 0.2962195612117648\n",
            "Epoch 3, Batch 1400, Loss: 0.294346593990922\n",
            "Epoch 3, Batch 1500, Loss: 0.30248420126736164\n",
            "Validation Loss: 0.2818848868310452\n",
            "Epoch 4, Batch 100, Loss: 0.29742235705256465\n",
            "Epoch 4, Batch 200, Loss: 0.30124646082520484\n",
            "Epoch 4, Batch 300, Loss: 0.27552474826574325\n",
            "Epoch 4, Batch 400, Loss: 0.2588249839842319\n",
            "Epoch 4, Batch 500, Loss: 0.28151278853416445\n",
            "Epoch 4, Batch 600, Loss: 0.28560450941324234\n",
            "Epoch 4, Batch 700, Loss: 0.2583820851147175\n",
            "Epoch 4, Batch 800, Loss: 0.2374550910666585\n",
            "Epoch 4, Batch 900, Loss: 0.23670888662338258\n",
            "Epoch 4, Batch 1000, Loss: 0.24892127498984337\n",
            "Epoch 4, Batch 1100, Loss: 0.2536622426658869\n",
            "Epoch 4, Batch 1200, Loss: 0.24609239779412748\n",
            "Epoch 4, Batch 1300, Loss: 0.2274858170375228\n",
            "Epoch 4, Batch 1400, Loss: 0.25073619090020655\n",
            "Epoch 4, Batch 1500, Loss: 0.24302812937647103\n",
            "Validation Loss: 0.2474589298615853\n",
            "Epoch 5, Batch 100, Loss: 0.2306697314977646\n",
            "Epoch 5, Batch 200, Loss: 0.22965276606380938\n",
            "Epoch 5, Batch 300, Loss: 0.22135918382555247\n",
            "Epoch 5, Batch 400, Loss: 0.23564674362540244\n",
            "Epoch 5, Batch 500, Loss: 0.2362766019254923\n",
            "Epoch 5, Batch 600, Loss: 0.23427065797150134\n",
            "Epoch 5, Batch 700, Loss: 0.21632698696106673\n",
            "Epoch 5, Batch 800, Loss: 0.2283000084757805\n",
            "Epoch 5, Batch 900, Loss: 0.22554607577621938\n",
            "Epoch 5, Batch 1000, Loss: 0.21247314900159836\n",
            "Epoch 5, Batch 1100, Loss: 0.23508198153227566\n",
            "Epoch 5, Batch 1200, Loss: 0.22379418630152942\n",
            "Epoch 5, Batch 1300, Loss: 0.22976637419313192\n",
            "Epoch 5, Batch 1400, Loss: 0.20399429569020866\n",
            "Epoch 5, Batch 1500, Loss: 0.21990376878529788\n",
            "Validation Loss: 0.21329664607346058\n",
            "Epoch 6, Batch 100, Loss: 0.2103218930400908\n",
            "Epoch 6, Batch 200, Loss: 0.20166724797338248\n",
            "Epoch 6, Batch 300, Loss: 0.19541215535253287\n",
            "Epoch 6, Batch 400, Loss: 0.21575507801026106\n",
            "Epoch 6, Batch 500, Loss: 0.19365763686597348\n",
            "Epoch 6, Batch 600, Loss: 0.19559525245800613\n",
            "Epoch 6, Batch 700, Loss: 0.20855114093050361\n",
            "Epoch 6, Batch 800, Loss: 0.20115907106548547\n",
            "Epoch 6, Batch 900, Loss: 0.19728279689326883\n",
            "Epoch 6, Batch 1000, Loss: 0.1962530621420592\n",
            "Epoch 6, Batch 1100, Loss: 0.19914048340171575\n",
            "Epoch 6, Batch 1200, Loss: 0.21271811405196786\n",
            "Epoch 6, Batch 1300, Loss: 0.1950840418972075\n",
            "Epoch 6, Batch 1400, Loss: 0.1954150016605854\n",
            "Epoch 6, Batch 1500, Loss: 0.19148730084300042\n",
            "Validation Loss: 0.2098142505188783\n",
            "Epoch 7, Batch 100, Loss: 0.1916329824924469\n",
            "Epoch 7, Batch 200, Loss: 0.17973040798678994\n",
            "Epoch 7, Batch 300, Loss: 0.18524860173463822\n",
            "Epoch 7, Batch 400, Loss: 0.1850968300551176\n",
            "Epoch 7, Batch 500, Loss: 0.2090254757925868\n",
            "Epoch 7, Batch 600, Loss: 0.18040353402495385\n",
            "Epoch 7, Batch 700, Loss: 0.19973792777396737\n",
            "Epoch 7, Batch 800, Loss: 0.18113401228561998\n",
            "Epoch 7, Batch 900, Loss: 0.1826078929565847\n",
            "Epoch 7, Batch 1000, Loss: 0.17865927197039128\n",
            "Epoch 7, Batch 1100, Loss: 0.1655477940104902\n",
            "Epoch 7, Batch 1200, Loss: 0.15374386057257652\n",
            "Epoch 7, Batch 1300, Loss: 0.2058032667450607\n",
            "Epoch 7, Batch 1400, Loss: 0.17199594788253308\n",
            "Epoch 7, Batch 1500, Loss: 0.17262303244322538\n",
            "Validation Loss: 0.18044593825936317\n",
            "Epoch 8, Batch 100, Loss: 0.17855861300602555\n",
            "Epoch 8, Batch 200, Loss: 0.1634651917219162\n",
            "Epoch 8, Batch 300, Loss: 0.16247064977884293\n",
            "Epoch 8, Batch 400, Loss: 0.16417257480323313\n",
            "Epoch 8, Batch 500, Loss: 0.17287514988332986\n",
            "Epoch 8, Batch 600, Loss: 0.16649107370525598\n",
            "Epoch 8, Batch 700, Loss: 0.14363629339262843\n",
            "Epoch 8, Batch 800, Loss: 0.1787884583696723\n",
            "Epoch 8, Batch 900, Loss: 0.16214806988835334\n",
            "Epoch 8, Batch 1000, Loss: 0.1604767518863082\n",
            "Epoch 8, Batch 1100, Loss: 0.1540369299054146\n",
            "Epoch 8, Batch 1200, Loss: 0.15431754736229777\n",
            "Epoch 8, Batch 1300, Loss: 0.19148659655824304\n",
            "Epoch 8, Batch 1400, Loss: 0.1698414883390069\n",
            "Epoch 8, Batch 1500, Loss: 0.16587158342823385\n",
            "Validation Loss: 0.16391084987918536\n",
            "Epoch 9, Batch 100, Loss: 0.1744444804638624\n",
            "Epoch 9, Batch 200, Loss: 0.16137574411928654\n",
            "Epoch 9, Batch 300, Loss: 0.15582408213987947\n",
            "Epoch 9, Batch 400, Loss: 0.15766429677605628\n",
            "Epoch 9, Batch 500, Loss: 0.17367633642628788\n",
            "Epoch 9, Batch 600, Loss: 0.13991899508982897\n",
            "Epoch 9, Batch 700, Loss: 0.1552165406383574\n",
            "Epoch 9, Batch 800, Loss: 0.17819037072360516\n",
            "Epoch 9, Batch 900, Loss: 0.14472773093730212\n",
            "Epoch 9, Batch 1000, Loss: 0.15368617402389645\n",
            "Epoch 9, Batch 1100, Loss: 0.13837013583630323\n",
            "Epoch 9, Batch 1200, Loss: 0.1428472436964512\n",
            "Epoch 9, Batch 1300, Loss: 0.15768477569334208\n",
            "Epoch 9, Batch 1400, Loss: 0.1414402387663722\n",
            "Epoch 9, Batch 1500, Loss: 0.15008771259337664\n",
            "Validation Loss: 0.16770619494716327\n",
            "Epoch 10, Batch 100, Loss: 0.1403084021806717\n",
            "Epoch 10, Batch 200, Loss: 0.14731475168839098\n",
            "Epoch 10, Batch 300, Loss: 0.14524948937818408\n",
            "Epoch 10, Batch 400, Loss: 0.12423053149133921\n",
            "Epoch 10, Batch 500, Loss: 0.14657368579879404\n",
            "Epoch 10, Batch 600, Loss: 0.14516727359499781\n",
            "Epoch 10, Batch 700, Loss: 0.14178072782233359\n",
            "Epoch 10, Batch 800, Loss: 0.14663646494038404\n",
            "Epoch 10, Batch 900, Loss: 0.13380460776388645\n",
            "Epoch 10, Batch 1000, Loss: 0.14722173856571316\n",
            "Epoch 10, Batch 1100, Loss: 0.1338513723015785\n",
            "Epoch 10, Batch 1200, Loss: 0.1636066753230989\n",
            "Epoch 10, Batch 1300, Loss: 0.148110217936337\n",
            "Epoch 10, Batch 1400, Loss: 0.1672318003512919\n",
            "Epoch 10, Batch 1500, Loss: 0.14524159802123904\n",
            "Validation Loss: 0.14472389347602924\n",
            "Epoch 11, Batch 100, Loss: 0.14407757738605143\n",
            "Epoch 11, Batch 200, Loss: 0.14072976876050233\n",
            "Epoch 11, Batch 300, Loss: 0.1662469534575939\n",
            "Epoch 11, Batch 400, Loss: 0.1258585776295513\n",
            "Epoch 11, Batch 500, Loss: 0.1341353881545365\n",
            "Epoch 11, Batch 600, Loss: 0.14616641711443662\n",
            "Epoch 11, Batch 700, Loss: 0.1429721790086478\n",
            "Epoch 11, Batch 800, Loss: 0.14631726348772645\n",
            "Epoch 11, Batch 900, Loss: 0.14268357275053858\n",
            "Epoch 11, Batch 1000, Loss: 0.1406821956951171\n",
            "Epoch 11, Batch 1100, Loss: 0.14297829382121563\n",
            "Epoch 11, Batch 1200, Loss: 0.13169180320575832\n",
            "Epoch 11, Batch 1300, Loss: 0.13732639722991735\n",
            "Epoch 11, Batch 1400, Loss: 0.12943061276338994\n",
            "Epoch 11, Batch 1500, Loss: 0.1431661849282682\n",
            "Validation Loss: 0.15674264784157277\n",
            "Epoch 12, Batch 100, Loss: 0.12148996636271477\n",
            "Epoch 12, Batch 200, Loss: 0.1274989780038595\n",
            "Epoch 12, Batch 300, Loss: 0.12308855144307017\n",
            "Epoch 12, Batch 400, Loss: 0.13669337248429655\n",
            "Epoch 12, Batch 500, Loss: 0.12458734915591776\n",
            "Epoch 12, Batch 600, Loss: 0.12874961487017572\n",
            "Epoch 12, Batch 700, Loss: 0.13262839622795583\n",
            "Epoch 12, Batch 800, Loss: 0.13090202685445548\n",
            "Epoch 12, Batch 900, Loss: 0.13043553770519792\n",
            "Epoch 12, Batch 1000, Loss: 0.13077217888087034\n",
            "Epoch 12, Batch 1100, Loss: 0.12471001324243844\n",
            "Epoch 12, Batch 1200, Loss: 0.14485559921711683\n",
            "Epoch 12, Batch 1300, Loss: 0.1506584930047393\n",
            "Epoch 12, Batch 1400, Loss: 0.13841339372098446\n",
            "Epoch 12, Batch 1500, Loss: 0.11499437996186317\n",
            "Validation Loss: 0.13360341297835113\n",
            "Epoch 13, Batch 100, Loss: 0.10962939232587815\n",
            "Epoch 13, Batch 200, Loss: 0.12158266004174947\n",
            "Epoch 13, Batch 300, Loss: 0.13703098310157658\n",
            "Epoch 13, Batch 400, Loss: 0.12168751935474574\n",
            "Epoch 13, Batch 500, Loss: 0.1357758234348148\n",
            "Epoch 13, Batch 600, Loss: 0.13037328344769775\n",
            "Epoch 13, Batch 700, Loss: 0.1208432610053569\n",
            "Epoch 13, Batch 800, Loss: 0.11546502297744154\n",
            "Epoch 13, Batch 900, Loss: 0.11349659466184675\n",
            "Epoch 13, Batch 1000, Loss: 0.12970450180582702\n",
            "Epoch 13, Batch 1100, Loss: 0.13719830407761038\n",
            "Epoch 13, Batch 1200, Loss: 0.13118007676675916\n",
            "Epoch 13, Batch 1300, Loss: 0.13070342609658836\n",
            "Epoch 13, Batch 1400, Loss: 0.12033001116476953\n",
            "Epoch 13, Batch 1500, Loss: 0.12499974122270942\n",
            "Validation Loss: 0.12554255913943052\n",
            "Epoch 14, Batch 100, Loss: 0.14154707983136178\n",
            "Epoch 14, Batch 200, Loss: 0.116239616968669\n",
            "Epoch 14, Batch 300, Loss: 0.1268128817155957\n",
            "Epoch 14, Batch 400, Loss: 0.10930248626507819\n",
            "Epoch 14, Batch 500, Loss: 0.09783167294226587\n",
            "Epoch 14, Batch 600, Loss: 0.12315131946466863\n",
            "Epoch 14, Batch 700, Loss: 0.12552351908758283\n",
            "Epoch 14, Batch 800, Loss: 0.11474198928102851\n",
            "Epoch 14, Batch 900, Loss: 0.12915939115919173\n",
            "Epoch 14, Batch 1000, Loss: 0.12261727690696717\n",
            "Epoch 14, Batch 1100, Loss: 0.11056222399696708\n",
            "Epoch 14, Batch 1200, Loss: 0.12974176324903966\n",
            "Epoch 14, Batch 1300, Loss: 0.12981423714198173\n",
            "Epoch 14, Batch 1400, Loss: 0.12927553434856237\n",
            "Epoch 14, Batch 1500, Loss: 0.11057012526318431\n",
            "Validation Loss: 0.13480377111087244\n",
            "Epoch 15, Batch 100, Loss: 0.11153276532888412\n",
            "Epoch 15, Batch 200, Loss: 0.11140947179868817\n",
            "Epoch 15, Batch 300, Loss: 0.1121077538933605\n",
            "Epoch 15, Batch 400, Loss: 0.11994298612698913\n",
            "Epoch 15, Batch 500, Loss: 0.11716285463422536\n",
            "Epoch 15, Batch 600, Loss: 0.12963768057525157\n",
            "Epoch 15, Batch 700, Loss: 0.12090862881857901\n",
            "Epoch 15, Batch 800, Loss: 0.1018028090428561\n",
            "Epoch 15, Batch 900, Loss: 0.10289108404889703\n",
            "Epoch 15, Batch 1000, Loss: 0.12009382701013237\n",
            "Epoch 15, Batch 1100, Loss: 0.12352421762421727\n",
            "Epoch 15, Batch 1200, Loss: 0.11627657499164343\n",
            "Epoch 15, Batch 1300, Loss: 0.10924987616483123\n",
            "Epoch 15, Batch 1400, Loss: 0.11054172658361494\n",
            "Epoch 15, Batch 1500, Loss: 0.11974747411441058\n",
            "Validation Loss: 0.12703394504388174\n",
            "Epoch 16, Batch 100, Loss: 0.09937908875755966\n",
            "Epoch 16, Batch 200, Loss: 0.10783813641406596\n",
            "Epoch 16, Batch 300, Loss: 0.12539159469306468\n",
            "Epoch 16, Batch 400, Loss: 0.10657346568070353\n",
            "Epoch 16, Batch 500, Loss: 0.1098935572290793\n",
            "Epoch 16, Batch 600, Loss: 0.11364166268147528\n",
            "Epoch 16, Batch 700, Loss: 0.1331074748095125\n",
            "Epoch 16, Batch 800, Loss: 0.12273072904907167\n",
            "Epoch 16, Batch 900, Loss: 0.1066851474577561\n",
            "Epoch 16, Batch 1000, Loss: 0.09994957273826004\n",
            "Epoch 16, Batch 1100, Loss: 0.11365233551710845\n",
            "Epoch 16, Batch 1200, Loss: 0.11457731134723873\n",
            "Epoch 16, Batch 1300, Loss: 0.10750951428897679\n",
            "Epoch 16, Batch 1400, Loss: 0.12588195864111185\n",
            "Epoch 16, Batch 1500, Loss: 0.11019018387421965\n",
            "Validation Loss: 0.1147314175280432\n",
            "Epoch 17, Batch 100, Loss: 0.10448980228975416\n",
            "Epoch 17, Batch 200, Loss: 0.10321746428031474\n",
            "Epoch 17, Batch 300, Loss: 0.10660033618099987\n",
            "Epoch 17, Batch 400, Loss: 0.10722232138272375\n",
            "Epoch 17, Batch 500, Loss: 0.09877867199946194\n",
            "Epoch 17, Batch 600, Loss: 0.12522925168275834\n",
            "Epoch 17, Batch 700, Loss: 0.1053026023413986\n",
            "Epoch 17, Batch 800, Loss: 0.11101907968521119\n",
            "Epoch 17, Batch 900, Loss: 0.11169298997148872\n",
            "Epoch 17, Batch 1000, Loss: 0.11931581852491945\n",
            "Epoch 17, Batch 1100, Loss: 0.12070374082773924\n",
            "Epoch 17, Batch 1200, Loss: 0.10211713430006057\n",
            "Epoch 17, Batch 1300, Loss: 0.09981199112720787\n",
            "Epoch 17, Batch 1400, Loss: 0.10917478353716432\n",
            "Epoch 17, Batch 1500, Loss: 0.11820374155417085\n",
            "Validation Loss: 0.1259127598516643\n",
            "Epoch 18, Batch 100, Loss: 0.10185713052749634\n",
            "Epoch 18, Batch 200, Loss: 0.11282397129572928\n",
            "Epoch 18, Batch 300, Loss: 0.0889167317817919\n",
            "Epoch 18, Batch 400, Loss: 0.09469658259302377\n",
            "Epoch 18, Batch 500, Loss: 0.09992663904093206\n",
            "Epoch 18, Batch 600, Loss: 0.09933509400580079\n",
            "Epoch 18, Batch 700, Loss: 0.10466912996023893\n",
            "Epoch 18, Batch 800, Loss: 0.12144986026920378\n",
            "Epoch 18, Batch 900, Loss: 0.10118164241779595\n",
            "Epoch 18, Batch 1000, Loss: 0.11190122653497384\n",
            "Epoch 18, Batch 1100, Loss: 0.09672487223986537\n",
            "Epoch 18, Batch 1200, Loss: 0.11786071723792702\n",
            "Epoch 18, Batch 1300, Loss: 0.09825229526730254\n",
            "Epoch 18, Batch 1400, Loss: 0.12209003741852939\n",
            "Epoch 18, Batch 1500, Loss: 0.10400235891342163\n",
            "Validation Loss: 0.1202453280997773\n",
            "Epoch 19, Batch 100, Loss: 0.08748992507811636\n",
            "Epoch 19, Batch 200, Loss: 0.11434680811595171\n",
            "Epoch 19, Batch 300, Loss: 0.08660377864260227\n",
            "Epoch 19, Batch 400, Loss: 0.10227521166205406\n",
            "Epoch 19, Batch 500, Loss: 0.10905636847019196\n",
            "Epoch 19, Batch 600, Loss: 0.09663555906154216\n",
            "Epoch 19, Batch 700, Loss: 0.11177261840552091\n",
            "Epoch 19, Batch 800, Loss: 0.10560151524841785\n",
            "Epoch 19, Batch 900, Loss: 0.09960179319605231\n",
            "Epoch 19, Batch 1000, Loss: 0.09707554046995938\n",
            "Epoch 19, Batch 1100, Loss: 0.10336832570377737\n",
            "Epoch 19, Batch 1200, Loss: 0.10068571561947465\n",
            "Epoch 19, Batch 1300, Loss: 0.10027612671256066\n",
            "Epoch 19, Batch 1400, Loss: 0.10448368683457375\n",
            "Epoch 19, Batch 1500, Loss: 0.10002419259399176\n",
            "Validation Loss: 0.11701572379842401\n",
            "Epoch 20, Batch 100, Loss: 0.08390339762438088\n",
            "Epoch 20, Batch 200, Loss: 0.09870870139915496\n",
            "Epoch 20, Batch 300, Loss: 0.09698581354692579\n",
            "Epoch 20, Batch 400, Loss: 0.10800074582919478\n",
            "Epoch 20, Batch 500, Loss: 0.11419860049150884\n",
            "Epoch 20, Batch 600, Loss: 0.10254919208586216\n",
            "Epoch 20, Batch 700, Loss: 0.10218826280441135\n",
            "Epoch 20, Batch 800, Loss: 0.08347751918248832\n",
            "Epoch 20, Batch 900, Loss: 0.11094987422227859\n",
            "Epoch 20, Batch 1000, Loss: 0.08726934559177607\n",
            "Epoch 20, Batch 1100, Loss: 0.10829297521151603\n",
            "Epoch 20, Batch 1200, Loss: 0.10326052123680711\n",
            "Epoch 20, Batch 1300, Loss: 0.10423385327216238\n",
            "Epoch 20, Batch 1400, Loss: 0.08575636074878275\n",
            "Epoch 20, Batch 1500, Loss: 0.12097883733455092\n",
            "Validation Loss: 0.13744633952900767\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "model = MLP([256, 256], [nn.ReLU(), nn.Tanh()]) # from best params above\n",
        "# 0.002, 0.001\n",
        "learning_rate = 0.017687904254669623\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001304789704792876, momentum=0.1894720749043393)\n",
        "\n",
        "# Data loaders with suggested batch size\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training the neural network\n",
        "# 3, 10, 20\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    running_loss = 0.0  # Reset running loss\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "    print(f'Validation Loss: {running_loss / len(val_loader)}')\n",
        "    model.train()  # Set model back to training mode\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfYr_0XlT-Cb",
        "outputId": "e298b73f-543d-49a6-dbf5-b3c0099fae75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.9581%\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "kNM8-ijzUE9w",
        "outputId": "a07af0ed-e731-461f-d8c1-35750b29394c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_index = 27\n",
        "test_image, test_label = test_dataset[image_index]\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(test_image.unsqueeze(0))\n",
        "    _, predicted_label = torch.max(output, 1)\n",
        "\n",
        "test_image_numpy = test_image.squeeze().numpy()\n",
        "\n",
        "plt.imshow(test_image_numpy, cmap='gray')\n",
        "plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoypxOXgGjuC"
      },
      "source": [
        "Notes for Part 1\n",
        "\n",
        "1. Activation function:\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))  # Change activation function here\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "2. loss function and optimizer\n",
        "\n",
        "model = Net()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Change loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "3. ~adding a dropout layer\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 128)\n",
        "        self.dropout = torch.nn.Dropout(0.2)  # Add a Dropout layer here\n",
        "        self.fc2 = torch.nn.Linear(128, 64)\n",
        "        self.fc3 = torch.nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply Dropout\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "4. model configurations / epochs\n",
        "\n",
        "epochs = 10  # Change number of epochs\n",
        "for epoch in range(epochs):\n",
        "    # Training loop\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # Training steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IIK5kzHGH0"
      },
      "source": [
        "## Task - 2\n",
        "\n",
        "### PyTorch FC ANN FMNIST Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_aWMlQ33ByRO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jOCzfnBvB_bQ"
      },
      "outputs": [],
      "source": [
        "# Transformations --> this is a \"pre-processing step\" that's typical for image processing methods\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.RandomRotation(10),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomVerticalFlip(),\n",
        "                                transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_val_split = 0.8\n",
        "\n",
        "# This dataset is already \"sorted\" as part of the import method, but no \"validation\" set has been selected in this case\n",
        "# Loading the FashionMNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Calculate sizes\n",
        "train_size = int(train_val_split * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jZk_FS9JCLOH"
      },
      "outputs": [],
      "source": [
        "# Mapping the labels for the FashionMNIST dataset -- later we'll see that this using the \"keras to_categorical\" method as discussed in class\n",
        "labels_map = {\n",
        "    0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
        "    5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "ykrRIGSdCMu5",
        "outputId": "a6737577-19d7-459e-aa83-ccebed56246c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#This cell is designed to display a few images from the dataset\n",
        "#It isn't necessary to run this, but it can help give a better idea of the challanges your model will face\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "\n",
        "# Displaying figures from the dataset randomly\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    img, label = train_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-10-16 15:20:41,046] A new study created in memory with name: no-name-60f62d75-dfae-46a3-b68a-4b177184032e\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_9416\\3238854038.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_9416\\3238854038.py:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
            "[I 2024-10-16 15:23:46,202] Trial 0 finished with value: 0.09916666666666667 and parameters: {'num_layers': 4, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'sigmoid', 'layer_size_2': 128, 'activation_2': 'tanh', 'layer_size_3': 128, 'activation_3': 'relu', 'optimizer': 'RMSprop', 'lr': 0.01874681270185393, 'weight_decay': 0.00628318607862972, 'batch_size': 128}. Best is trial 0 with value: 0.09916666666666667.\n",
            "[I 2024-10-16 15:26:52,908] Trial 1 finished with value: 0.47333333333333333 and parameters: {'num_layers': 4, 'layer_size_0': 32, 'activation_0': 'sigmoid', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'layer_size_2': 256, 'activation_2': 'tanh', 'layer_size_3': 64, 'activation_3': 'relu', 'optimizer': 'RMSprop', 'lr': 2.4435914449035458e-05, 'weight_decay': 0.000316486124516407, 'batch_size': 64}. Best is trial 1 with value: 0.47333333333333333.\n",
            "C:\\Users\\rgbmr\\AppData\\Local\\Temp\\ipykernel_9416\\3238854038.py:46: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  momentum = trial.suggest_uniform('momentum', 0.0, 0.9)\n",
            "[I 2024-10-16 15:29:59,563] Trial 2 finished with value: 0.7488333333333334 and parameters: {'num_layers': 3, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 64, 'activation_1': 'tanh', 'layer_size_2': 64, 'activation_2': 'sigmoid', 'optimizer': 'SGD', 'lr': 0.021038616940244056, 'weight_decay': 1.8613667460170704e-05, 'batch_size': 32, 'momentum': 0.4407965037939796}. Best is trial 2 with value: 0.7488333333333334.\n",
            "[I 2024-10-16 15:33:08,940] Trial 3 finished with value: 0.70975 and parameters: {'num_layers': 2, 'layer_size_0': 256, 'activation_0': 'tanh', 'layer_size_1': 128, 'activation_1': 'tanh', 'optimizer': 'RMSprop', 'lr': 0.002610571517425045, 'weight_decay': 2.7118812037741016e-06, 'batch_size': 32}. Best is trial 2 with value: 0.7488333333333334.\n",
            "[I 2024-10-16 15:36:17,281] Trial 4 finished with value: 0.7335 and parameters: {'num_layers': 3, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 256, 'activation_1': 'tanh', 'layer_size_2': 256, 'activation_2': 'relu', 'optimizer': 'RMSprop', 'lr': 7.083368127450415e-05, 'weight_decay': 5.413805963484687e-06, 'batch_size': 128}. Best is trial 2 with value: 0.7488333333333334.\n",
            "[I 2024-10-16 15:39:20,548] Trial 5 finished with value: 0.7525833333333334 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 0.07805877808715575, 'weight_decay': 2.238735227182741e-06, 'batch_size': 64, 'momentum': 0.7714533546098917}. Best is trial 5 with value: 0.7525833333333334.\n",
            "[I 2024-10-16 15:42:23,653] Trial 6 finished with value: 0.7060833333333333 and parameters: {'num_layers': 4, 'layer_size_0': 64, 'activation_0': 'relu', 'layer_size_1': 64, 'activation_1': 'tanh', 'layer_size_2': 32, 'activation_2': 'relu', 'layer_size_3': 128, 'activation_3': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.00032025790210728834, 'weight_decay': 0.00033538481098792627, 'batch_size': 128}. Best is trial 5 with value: 0.7525833333333334.\n",
            "[I 2024-10-16 15:45:26,551] Trial 7 finished with value: 0.10783333333333334 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'tanh', 'layer_size_1': 256, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 1.1530874515828898e-05, 'weight_decay': 0.0038770919320179477, 'batch_size': 128, 'momentum': 0.4019964398102689}. Best is trial 5 with value: 0.7525833333333334.\n",
            "[I 2024-10-16 15:48:34,761] Trial 8 finished with value: 0.78375 and parameters: {'num_layers': 2, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.001056853608332082, 'weight_decay': 5.301484414947403e-06, 'batch_size': 32}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 15:51:39,428] Trial 9 finished with value: 0.6895 and parameters: {'num_layers': 2, 'layer_size_0': 64, 'activation_0': 'relu', 'layer_size_1': 64, 'activation_1': 'relu', 'optimizer': 'Adam', 'lr': 0.011327742301239643, 'weight_decay': 8.687392375429997e-05, 'batch_size': 64}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 15:54:52,142] Trial 10 finished with value: 0.7418333333333333 and parameters: {'num_layers': 3, 'layer_size_0': 128, 'activation_0': 'sigmoid', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'layer_size_2': 32, 'activation_2': 'sigmoid', 'optimizer': 'Adam', 'lr': 0.0007079490046509285, 'weight_decay': 2.3694662704316025e-05, 'batch_size': 32}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 15:57:55,057] Trial 11 finished with value: 0.7094166666666667 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 0.06783516426765156, 'weight_decay': 1.0673806247316475e-06, 'batch_size': 64, 'momentum': 0.890008784289799}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:01:01,832] Trial 12 finished with value: 0.719 and parameters: {'num_layers': 2, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'sigmoid', 'optimizer': 'SGD', 'lr': 0.0027961467010640738, 'weight_decay': 7.932010773474843e-06, 'batch_size': 32, 'momentum': 0.8881340214975426}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:04:12,906] Trial 13 finished with value: 0.7494166666666666 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'relu', 'optimizer': 'SGD', 'lr': 0.09226827656581249, 'weight_decay': 1.2235272610824971e-06, 'batch_size': 64, 'momentum': 0.6575897804883629}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:07:38,202] Trial 14 finished with value: 0.675 and parameters: {'num_layers': 3, 'layer_size_0': 128, 'activation_0': 'tanh', 'layer_size_1': 32, 'activation_1': 'relu', 'layer_size_2': 128, 'activation_2': 'sigmoid', 'optimizer': 'Adam', 'lr': 0.004083283374038546, 'weight_decay': 3.2916462426239303e-05, 'batch_size': 32}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:10:55,339] Trial 15 finished with value: 0.63075 and parameters: {'num_layers': 2, 'layer_size_0': 32, 'activation_0': 'sigmoid', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.00015686269602581805, 'weight_decay': 4.903884430634879e-06, 'batch_size': 64}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:14:14,753] Trial 16 finished with value: 0.3591666666666667 and parameters: {'num_layers': 3, 'layer_size_0': 256, 'activation_0': 'relu', 'layer_size_1': 32, 'activation_1': 'relu', 'layer_size_2': 64, 'activation_2': 'relu', 'optimizer': 'SGD', 'lr': 0.0009656534023174905, 'weight_decay': 0.00010380494276315175, 'batch_size': 64, 'momentum': 0.009513247105838063}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:17:36,217] Trial 17 finished with value: 0.69775 and parameters: {'num_layers': 2, 'layer_size_0': 64, 'activation_0': 'relu', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.006603296691387565, 'weight_decay': 2.4222655232352715e-06, 'batch_size': 32}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:20:55,584] Trial 18 finished with value: 0.7185 and parameters: {'num_layers': 3, 'layer_size_0': 32, 'activation_0': 'sigmoid', 'layer_size_1': 32, 'activation_1': 'sigmoid', 'layer_size_2': 32, 'activation_2': 'tanh', 'optimizer': 'SGD', 'lr': 0.03864111708417515, 'weight_decay': 1.2676681004674881e-05, 'batch_size': 32, 'momentum': 0.6288845230285476}. Best is trial 8 with value: 0.78375.\n",
            "[I 2024-10-16 16:24:14,919] Trial 19 finished with value: 0.7286666666666667 and parameters: {'num_layers': 2, 'layer_size_0': 128, 'activation_0': 'tanh', 'layer_size_1': 128, 'activation_1': 'relu', 'optimizer': 'Adam', 'lr': 6.0466005522287746e-05, 'weight_decay': 8.236436851014117e-05, 'batch_size': 64}. Best is trial 8 with value: 0.78375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'num_layers': 2, 'layer_size_0': 128, 'activation_0': 'relu', 'layer_size_1': 128, 'activation_1': 'sigmoid', 'optimizer': 'RMSprop', 'lr': 0.001056853608332082, 'weight_decay': 5.301484414947403e-06, 'batch_size': 32}\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Best hyperparameters:', study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hFOEXCPEVTw",
        "outputId": "6c4d33e6-8624-4eae-d51b-7008e6826780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.3447680562734603\n",
            "Epoch 1, Batch 200, Loss: 0.9830856829881668\n",
            "Epoch 1, Batch 300, Loss: 0.8845025140047074\n",
            "Epoch 1, Batch 400, Loss: 0.8360383078455925\n",
            "Epoch 1, Batch 500, Loss: 0.7992652341723442\n",
            "Epoch 1, Batch 600, Loss: 0.7849914020299912\n",
            "Epoch 1, Batch 700, Loss: 0.782193575501442\n",
            "Epoch 1, Batch 800, Loss: 0.7744410452246666\n",
            "Epoch 1, Batch 900, Loss: 0.7453978726267815\n",
            "Epoch 1, Batch 1000, Loss: 0.7717109832167626\n",
            "Epoch 1, Batch 1100, Loss: 0.7216164082288742\n",
            "Epoch 1, Batch 1200, Loss: 0.7698494336009025\n",
            "Epoch 1, Batch 1300, Loss: 0.7284211531281471\n",
            "Epoch 1, Batch 1400, Loss: 0.7392239198088646\n",
            "Epoch 1, Batch 1500, Loss: 0.7381336161494255\n",
            "Validation Loss: 0.7300344111919403\n",
            "Epoch 2, Batch 100, Loss: 0.7179026398062706\n",
            "Epoch 2, Batch 200, Loss: 0.6826613938808441\n",
            "Epoch 2, Batch 300, Loss: 0.7065676474571227\n",
            "Epoch 2, Batch 400, Loss: 0.6843813115358353\n",
            "Epoch 2, Batch 500, Loss: 0.6885463917255401\n",
            "Epoch 2, Batch 600, Loss: 0.6924367928504944\n",
            "Epoch 2, Batch 700, Loss: 0.6901946556568146\n",
            "Epoch 2, Batch 800, Loss: 0.6779610899090767\n",
            "Epoch 2, Batch 900, Loss: 0.6975444599986076\n",
            "Epoch 2, Batch 1000, Loss: 0.6734490570425987\n",
            "Epoch 2, Batch 1100, Loss: 0.6540196326375007\n",
            "Epoch 2, Batch 1200, Loss: 0.6508120375871659\n",
            "Epoch 2, Batch 1300, Loss: 0.6385590893030166\n",
            "Epoch 2, Batch 1400, Loss: 0.6695941486954688\n",
            "Epoch 2, Batch 1500, Loss: 0.662493616938591\n",
            "Validation Loss: 0.6477389304637909\n",
            "Epoch 3, Batch 100, Loss: 0.6102698788046836\n",
            "Epoch 3, Batch 200, Loss: 0.646453403532505\n",
            "Epoch 3, Batch 300, Loss: 0.6475850287079811\n",
            "Epoch 3, Batch 400, Loss: 0.6137565287947655\n",
            "Epoch 3, Batch 500, Loss: 0.6207957762479782\n",
            "Epoch 3, Batch 600, Loss: 0.6381102776527405\n",
            "Epoch 3, Batch 700, Loss: 0.6245834717154503\n",
            "Epoch 3, Batch 800, Loss: 0.6220415219664573\n",
            "Epoch 3, Batch 900, Loss: 0.620283767580986\n",
            "Epoch 3, Batch 1000, Loss: 0.6159866741299629\n",
            "Epoch 3, Batch 1100, Loss: 0.5924212867021561\n",
            "Epoch 3, Batch 1200, Loss: 0.5968965783715248\n",
            "Epoch 3, Batch 1300, Loss: 0.5966349628567695\n",
            "Epoch 3, Batch 1400, Loss: 0.6273028853535653\n",
            "Epoch 3, Batch 1500, Loss: 0.630255226790905\n",
            "Validation Loss: 0.6210061530272166\n",
            "Epoch 4, Batch 100, Loss: 0.6046444422006607\n",
            "Epoch 4, Batch 200, Loss: 0.5813604229688645\n",
            "Epoch 4, Batch 300, Loss: 0.6195009830594063\n",
            "Epoch 4, Batch 400, Loss: 0.6249835342168808\n",
            "Epoch 4, Batch 500, Loss: 0.587856361567974\n",
            "Epoch 4, Batch 600, Loss: 0.6148089450597763\n",
            "Epoch 4, Batch 700, Loss: 0.5885165140032769\n",
            "Epoch 4, Batch 800, Loss: 0.6370405997335911\n",
            "Epoch 4, Batch 900, Loss: 0.5576790784299374\n",
            "Epoch 4, Batch 1000, Loss: 0.5894360437989234\n",
            "Epoch 4, Batch 1100, Loss: 0.6054818859696388\n",
            "Epoch 4, Batch 1200, Loss: 0.5656589703261852\n",
            "Epoch 4, Batch 1300, Loss: 0.569205237030983\n",
            "Epoch 4, Batch 1400, Loss: 0.5775784751772881\n",
            "Epoch 4, Batch 1500, Loss: 0.5642776934802533\n",
            "Validation Loss: 0.6282610375881195\n",
            "Epoch 5, Batch 100, Loss: 0.5767564013600349\n",
            "Epoch 5, Batch 200, Loss: 0.5841266715526581\n",
            "Epoch 5, Batch 300, Loss: 0.5738670645654201\n",
            "Epoch 5, Batch 400, Loss: 0.6112201845645905\n",
            "Epoch 5, Batch 500, Loss: 0.5604986354708672\n",
            "Epoch 5, Batch 600, Loss: 0.5534411670267582\n",
            "Epoch 5, Batch 700, Loss: 0.5655572432279586\n",
            "Epoch 5, Batch 800, Loss: 0.5599841839075088\n",
            "Epoch 5, Batch 900, Loss: 0.5762295296788216\n",
            "Epoch 5, Batch 1000, Loss: 0.5748713499307633\n",
            "Epoch 5, Batch 1100, Loss: 0.5812759700417519\n",
            "Epoch 5, Batch 1200, Loss: 0.5682052947580815\n",
            "Epoch 5, Batch 1300, Loss: 0.5570125937461853\n",
            "Epoch 5, Batch 1400, Loss: 0.5617663538455964\n",
            "Epoch 5, Batch 1500, Loss: 0.5562410134077073\n",
            "Validation Loss: 0.5815036614338557\n",
            "Epoch 6, Batch 100, Loss: 0.5585903257131577\n",
            "Epoch 6, Batch 200, Loss: 0.5682570445537567\n",
            "Epoch 6, Batch 300, Loss: 0.5512612864375115\n",
            "Epoch 6, Batch 400, Loss: 0.5862065467238426\n",
            "Epoch 6, Batch 500, Loss: 0.551706753373146\n",
            "Epoch 6, Batch 600, Loss: 0.568085648715496\n",
            "Epoch 6, Batch 700, Loss: 0.5421878534555435\n",
            "Epoch 6, Batch 800, Loss: 0.5540836554765701\n",
            "Epoch 6, Batch 900, Loss: 0.566713753938675\n",
            "Epoch 6, Batch 1000, Loss: 0.5362316723167896\n",
            "Epoch 6, Batch 1100, Loss: 0.5499158126115798\n",
            "Epoch 6, Batch 1200, Loss: 0.5324400082230568\n",
            "Epoch 6, Batch 1300, Loss: 0.561794254630804\n",
            "Epoch 6, Batch 1400, Loss: 0.5483912378549576\n",
            "Epoch 6, Batch 1500, Loss: 0.5182682254910469\n",
            "Validation Loss: 0.5659904220501581\n",
            "Epoch 7, Batch 100, Loss: 0.5213973557949066\n",
            "Epoch 7, Batch 200, Loss: 0.5860043381154537\n",
            "Epoch 7, Batch 300, Loss: 0.5378137955069542\n",
            "Epoch 7, Batch 400, Loss: 0.5494200088083744\n",
            "Epoch 7, Batch 500, Loss: 0.553675220310688\n",
            "Epoch 7, Batch 600, Loss: 0.561515801846981\n",
            "Epoch 7, Batch 700, Loss: 0.5185397347807884\n",
            "Epoch 7, Batch 800, Loss: 0.5509960186481476\n",
            "Epoch 7, Batch 900, Loss: 0.5281783959269524\n",
            "Epoch 7, Batch 1000, Loss: 0.5513043539971113\n",
            "Epoch 7, Batch 1100, Loss: 0.5339703726768493\n",
            "Epoch 7, Batch 1200, Loss: 0.5234492962062359\n",
            "Epoch 7, Batch 1300, Loss: 0.5378775200247765\n",
            "Epoch 7, Batch 1400, Loss: 0.546897077858448\n",
            "Epoch 7, Batch 1500, Loss: 0.5224127006530762\n",
            "Validation Loss: 0.5352578153610229\n",
            "Epoch 8, Batch 100, Loss: 0.5223932839930058\n",
            "Epoch 8, Batch 200, Loss: 0.5280442859232426\n",
            "Epoch 8, Batch 300, Loss: 0.5473093818128109\n",
            "Epoch 8, Batch 400, Loss: 0.5485037507116794\n",
            "Epoch 8, Batch 500, Loss: 0.5521987093985081\n",
            "Epoch 8, Batch 600, Loss: 0.51656628921628\n",
            "Epoch 8, Batch 700, Loss: 0.5164804971218109\n",
            "Epoch 8, Batch 800, Loss: 0.5405032005906105\n",
            "Epoch 8, Batch 900, Loss: 0.5526036602258683\n",
            "Epoch 8, Batch 1000, Loss: 0.5249146756529808\n",
            "Epoch 8, Batch 1100, Loss: 0.5306652605533599\n",
            "Epoch 8, Batch 1200, Loss: 0.547920997440815\n",
            "Epoch 8, Batch 1300, Loss: 0.538049358278513\n",
            "Epoch 8, Batch 1400, Loss: 0.5266201043128967\n",
            "Epoch 8, Batch 1500, Loss: 0.5204026579856873\n",
            "Validation Loss: 0.5526632977326711\n",
            "Epoch 9, Batch 100, Loss: 0.541072033047676\n",
            "Epoch 9, Batch 200, Loss: 0.5207804344594479\n",
            "Epoch 9, Batch 300, Loss: 0.5224548037350177\n",
            "Epoch 9, Batch 400, Loss: 0.5118908675014973\n",
            "Epoch 9, Batch 500, Loss: 0.5163693451881408\n",
            "Epoch 9, Batch 600, Loss: 0.5197999048233032\n",
            "Epoch 9, Batch 700, Loss: 0.524998351931572\n",
            "Epoch 9, Batch 800, Loss: 0.5244240628182888\n",
            "Epoch 9, Batch 900, Loss: 0.5073427657783032\n",
            "Epoch 9, Batch 1000, Loss: 0.5385446055233478\n",
            "Epoch 9, Batch 1100, Loss: 0.5149522517621518\n",
            "Epoch 9, Batch 1200, Loss: 0.5124903559684754\n",
            "Epoch 9, Batch 1300, Loss: 0.5282919058203697\n",
            "Epoch 9, Batch 1400, Loss: 0.5022605188190937\n",
            "Epoch 9, Batch 1500, Loss: 0.5591178722679615\n",
            "Validation Loss: 0.5256798640489578\n",
            "Epoch 10, Batch 100, Loss: 0.5322798745334149\n",
            "Epoch 10, Batch 200, Loss: 0.4965425643324852\n",
            "Epoch 10, Batch 300, Loss: 0.5008205015957355\n",
            "Epoch 10, Batch 400, Loss: 0.507363021671772\n",
            "Epoch 10, Batch 500, Loss: 0.5273100332915783\n",
            "Epoch 10, Batch 600, Loss: 0.5469035091996193\n",
            "Epoch 10, Batch 700, Loss: 0.5371740908920765\n",
            "Epoch 10, Batch 800, Loss: 0.5145865851640701\n",
            "Epoch 10, Batch 900, Loss: 0.523264714628458\n",
            "Epoch 10, Batch 1000, Loss: 0.5021472637355328\n",
            "Epoch 10, Batch 1100, Loss: 0.5172545766830444\n",
            "Epoch 10, Batch 1200, Loss: 0.4929970233142376\n",
            "Epoch 10, Batch 1300, Loss: 0.5102917309105396\n",
            "Epoch 10, Batch 1400, Loss: 0.5508769555389881\n",
            "Epoch 10, Batch 1500, Loss: 0.5010119645297527\n",
            "Validation Loss: 0.5392206522226334\n",
            "Epoch 11, Batch 100, Loss: 0.5402750384807586\n",
            "Epoch 11, Batch 200, Loss: 0.5127814176678658\n",
            "Epoch 11, Batch 300, Loss: 0.5137998849153519\n",
            "Epoch 11, Batch 400, Loss: 0.5050850285589695\n",
            "Epoch 11, Batch 500, Loss: 0.5142248380184173\n",
            "Epoch 11, Batch 600, Loss: 0.5108137910068035\n",
            "Epoch 11, Batch 700, Loss: 0.48885036423802375\n",
            "Epoch 11, Batch 800, Loss: 0.530577552318573\n",
            "Epoch 11, Batch 900, Loss: 0.5041574357450008\n",
            "Epoch 11, Batch 1000, Loss: 0.5011912083625794\n",
            "Epoch 11, Batch 1100, Loss: 0.5322251170873642\n",
            "Epoch 11, Batch 1200, Loss: 0.5129690206050873\n",
            "Epoch 11, Batch 1300, Loss: 0.5139107652008533\n",
            "Epoch 11, Batch 1400, Loss: 0.5273787838220596\n",
            "Epoch 11, Batch 1500, Loss: 0.5225168520212173\n",
            "Validation Loss: 0.5152123153209687\n",
            "Epoch 12, Batch 100, Loss: 0.5082567396759987\n",
            "Epoch 12, Batch 200, Loss: 0.5175756092369557\n",
            "Epoch 12, Batch 300, Loss: 0.4866683229804039\n",
            "Epoch 12, Batch 400, Loss: 0.5059609143435955\n",
            "Epoch 12, Batch 500, Loss: 0.5019188299775124\n",
            "Epoch 12, Batch 600, Loss: 0.5144008512794972\n",
            "Epoch 12, Batch 700, Loss: 0.48774257898330686\n",
            "Epoch 12, Batch 800, Loss: 0.5111542150378228\n",
            "Epoch 12, Batch 900, Loss: 0.5047628648579121\n",
            "Epoch 12, Batch 1000, Loss: 0.4984069430828095\n",
            "Epoch 12, Batch 1100, Loss: 0.518295855820179\n",
            "Epoch 12, Batch 1200, Loss: 0.49932418212294577\n",
            "Epoch 12, Batch 1300, Loss: 0.5002641974389553\n",
            "Epoch 12, Batch 1400, Loss: 0.514155051112175\n",
            "Epoch 12, Batch 1500, Loss: 0.5084473736584186\n",
            "Validation Loss: 0.5086495085159938\n",
            "Epoch 13, Batch 100, Loss: 0.5104171414673329\n",
            "Epoch 13, Batch 200, Loss: 0.4859697476029396\n",
            "Epoch 13, Batch 300, Loss: 0.508984834253788\n",
            "Epoch 13, Batch 400, Loss: 0.49547489047050475\n",
            "Epoch 13, Batch 500, Loss: 0.5172116918861867\n",
            "Epoch 13, Batch 600, Loss: 0.4837892189621925\n",
            "Epoch 13, Batch 700, Loss: 0.5196664533019066\n",
            "Epoch 13, Batch 800, Loss: 0.5205754451453686\n",
            "Epoch 13, Batch 900, Loss: 0.5186822701990604\n",
            "Epoch 13, Batch 1000, Loss: 0.5006254413723945\n",
            "Epoch 13, Batch 1100, Loss: 0.5200230537354946\n",
            "Epoch 13, Batch 1200, Loss: 0.5338046675920487\n",
            "Epoch 13, Batch 1300, Loss: 0.4872722181677818\n",
            "Epoch 13, Batch 1400, Loss: 0.5032997012138367\n",
            "Epoch 13, Batch 1500, Loss: 0.4772633366286755\n",
            "Validation Loss: 0.5318216700553894\n",
            "Epoch 14, Batch 100, Loss: 0.5074181154370307\n",
            "Epoch 14, Batch 200, Loss: 0.4935022781789303\n",
            "Epoch 14, Batch 300, Loss: 0.5152598990499974\n",
            "Epoch 14, Batch 400, Loss: 0.5110115110874176\n",
            "Epoch 14, Batch 500, Loss: 0.4952529960870743\n",
            "Epoch 14, Batch 600, Loss: 0.507086285352707\n",
            "Epoch 14, Batch 700, Loss: 0.5137856546044349\n",
            "Epoch 14, Batch 800, Loss: 0.4839470183849335\n",
            "Epoch 14, Batch 900, Loss: 0.47920789033174516\n",
            "Epoch 14, Batch 1000, Loss: 0.49938931569457057\n",
            "Epoch 14, Batch 1100, Loss: 0.48972172051668167\n",
            "Epoch 14, Batch 1200, Loss: 0.4879798129200935\n",
            "Epoch 14, Batch 1300, Loss: 0.5117517198622227\n",
            "Epoch 14, Batch 1400, Loss: 0.48679631009697916\n",
            "Epoch 14, Batch 1500, Loss: 0.4957275573909283\n",
            "Validation Loss: 0.531580208937327\n",
            "Epoch 15, Batch 100, Loss: 0.4836513139307499\n",
            "Epoch 15, Batch 200, Loss: 0.5215611802041531\n",
            "Epoch 15, Batch 300, Loss: 0.49287824153900145\n",
            "Epoch 15, Batch 400, Loss: 0.487738411873579\n",
            "Epoch 15, Batch 500, Loss: 0.49624030649662015\n",
            "Epoch 15, Batch 600, Loss: 0.5250561065971852\n",
            "Epoch 15, Batch 700, Loss: 0.5067689292132854\n",
            "Epoch 15, Batch 800, Loss: 0.49172229632735254\n",
            "Epoch 15, Batch 900, Loss: 0.4691465759277344\n",
            "Epoch 15, Batch 1000, Loss: 0.49310120329260826\n",
            "Epoch 15, Batch 1100, Loss: 0.49844888255000114\n",
            "Epoch 15, Batch 1200, Loss: 0.47634750545024873\n",
            "Epoch 15, Batch 1300, Loss: 0.49926136881113053\n",
            "Epoch 15, Batch 1400, Loss: 0.5280693858861923\n",
            "Epoch 15, Batch 1500, Loss: 0.5046761083602905\n",
            "Validation Loss: 0.5021614811221758\n",
            "Epoch 16, Batch 100, Loss: 0.4905719220638275\n",
            "Epoch 16, Batch 200, Loss: 0.5084588234126568\n",
            "Epoch 16, Batch 300, Loss: 0.5131113459169865\n",
            "Epoch 16, Batch 400, Loss: 0.4988231168687344\n",
            "Epoch 16, Batch 500, Loss: 0.5092890293896198\n",
            "Epoch 16, Batch 600, Loss: 0.46864578634500503\n",
            "Epoch 16, Batch 700, Loss: 0.4978382229804993\n",
            "Epoch 16, Batch 800, Loss: 0.467242236584425\n",
            "Epoch 16, Batch 900, Loss: 0.49604478120803835\n",
            "Epoch 16, Batch 1000, Loss: 0.5073524007201194\n",
            "Epoch 16, Batch 1100, Loss: 0.506050910204649\n",
            "Epoch 16, Batch 1200, Loss: 0.4671826305985451\n",
            "Epoch 16, Batch 1300, Loss: 0.49242294281721116\n",
            "Epoch 16, Batch 1400, Loss: 0.4845756649971008\n",
            "Epoch 16, Batch 1500, Loss: 0.5045269848406315\n",
            "Validation Loss: 0.5009980421861012\n",
            "Epoch 17, Batch 100, Loss: 0.47988320477306845\n",
            "Epoch 17, Batch 200, Loss: 0.5021739166229964\n",
            "Epoch 17, Batch 300, Loss: 0.4995580492913723\n",
            "Epoch 17, Batch 400, Loss: 0.5241739967465401\n",
            "Epoch 17, Batch 500, Loss: 0.49544459909200667\n",
            "Epoch 17, Batch 600, Loss: 0.4785927106440067\n",
            "Epoch 17, Batch 700, Loss: 0.4717388254404068\n",
            "Epoch 17, Batch 800, Loss: 0.48436143338680265\n",
            "Epoch 17, Batch 900, Loss: 0.48272376984357834\n",
            "Epoch 17, Batch 1000, Loss: 0.5090857373178005\n",
            "Epoch 17, Batch 1100, Loss: 0.46515673607587815\n",
            "Epoch 17, Batch 1200, Loss: 0.4726467415690422\n",
            "Epoch 17, Batch 1300, Loss: 0.5051658004522324\n",
            "Epoch 17, Batch 1400, Loss: 0.4647768993675709\n",
            "Epoch 17, Batch 1500, Loss: 0.49559276252985\n",
            "Validation Loss: 0.5048895455201466\n",
            "Epoch 18, Batch 100, Loss: 0.48418335765600207\n",
            "Epoch 18, Batch 200, Loss: 0.49845956802368163\n",
            "Epoch 18, Batch 300, Loss: 0.4769114646315575\n",
            "Epoch 18, Batch 400, Loss: 0.4869718624651432\n",
            "Epoch 18, Batch 500, Loss: 0.4964540852606297\n",
            "Epoch 18, Batch 600, Loss: 0.49139827460050584\n",
            "Epoch 18, Batch 700, Loss: 0.49061016872525215\n",
            "Epoch 18, Batch 800, Loss: 0.5124676911532879\n",
            "Epoch 18, Batch 900, Loss: 0.4677189416438341\n",
            "Epoch 18, Batch 1000, Loss: 0.48783052355051043\n",
            "Epoch 18, Batch 1100, Loss: 0.48686440333724024\n",
            "Epoch 18, Batch 1200, Loss: 0.482871735394001\n",
            "Epoch 18, Batch 1300, Loss: 0.5019503769278526\n",
            "Epoch 18, Batch 1400, Loss: 0.4883598107099533\n",
            "Epoch 18, Batch 1500, Loss: 0.5071266286075116\n",
            "Validation Loss: 0.48762928597132366\n",
            "Epoch 19, Batch 100, Loss: 0.46670238152146337\n",
            "Epoch 19, Batch 200, Loss: 0.48177463322877884\n",
            "Epoch 19, Batch 300, Loss: 0.4996517868340015\n",
            "Epoch 19, Batch 400, Loss: 0.5021536284685135\n",
            "Epoch 19, Batch 500, Loss: 0.4742228065431118\n",
            "Epoch 19, Batch 600, Loss: 0.48985210329294204\n",
            "Epoch 19, Batch 700, Loss: 0.4883945481479168\n",
            "Epoch 19, Batch 800, Loss: 0.4861441503465176\n",
            "Epoch 19, Batch 900, Loss: 0.4689652064442635\n",
            "Epoch 19, Batch 1000, Loss: 0.4708311477303505\n",
            "Epoch 19, Batch 1100, Loss: 0.4902515225112438\n",
            "Epoch 19, Batch 1200, Loss: 0.48508078679442407\n",
            "Epoch 19, Batch 1300, Loss: 0.49022717237472535\n",
            "Epoch 19, Batch 1400, Loss: 0.48094677850604056\n",
            "Epoch 19, Batch 1500, Loss: 0.4763550353050232\n",
            "Validation Loss: 0.4929987159967423\n",
            "Epoch 20, Batch 100, Loss: 0.4855370160937309\n",
            "Epoch 20, Batch 200, Loss: 0.4928800654411316\n",
            "Epoch 20, Batch 300, Loss: 0.4615896998345852\n",
            "Epoch 20, Batch 400, Loss: 0.4710350288450718\n",
            "Epoch 20, Batch 500, Loss: 0.48569800809025765\n",
            "Epoch 20, Batch 600, Loss: 0.4897096924483776\n",
            "Epoch 20, Batch 700, Loss: 0.48106227919459343\n",
            "Epoch 20, Batch 800, Loss: 0.47897524267435077\n",
            "Epoch 20, Batch 900, Loss: 0.5082563053071499\n",
            "Epoch 20, Batch 1000, Loss: 0.4731269319355488\n",
            "Epoch 20, Batch 1100, Loss: 0.4785140389204025\n",
            "Epoch 20, Batch 1200, Loss: 0.513259729295969\n",
            "Epoch 20, Batch 1300, Loss: 0.4957345646619797\n",
            "Epoch 20, Batch 1400, Loss: 0.48024810388684275\n",
            "Epoch 20, Batch 1500, Loss: 0.4891945767402649\n",
            "Validation Loss: 0.4930850981871287\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "model = MLP([128, 128], [nn.ReLU(), nn.Sigmoid()]) # from best params above\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001056853608332082, weight_decay=5.301484414947403e-06)\n",
        "\n",
        "# Data loaders with suggested batch size\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training the neural network\n",
        "# 3, 10, 20\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    running_loss = 0.0  # Reset running loss\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "    print(f'Validation Loss: {running_loss / len(val_loader)}')\n",
        "    model.train()  # Set model back to training mode\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMCpk60EaXr",
        "outputId": "c8ffbf3e-bd21-4e29-8428-8405792a0d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.8062\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: { correct / total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "J2GkmLeQEeZV",
        "outputId": "cb5bae66-d3d2-4163-deef-5828aad6b068"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_index = 27\n",
        "test_image, test_label = test_dataset[image_index]\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(test_image.unsqueeze(0))\n",
        "    _, predicted_label = torch.max(output, 1)\n",
        "\n",
        "test_image_numpy = test_image.squeeze().numpy()\n",
        "\n",
        "plt.imshow(test_image_numpy, cmap='gray')\n",
        "plt.title(f'Predicted Label: {predicted_label.item()}, Actual Label: {test_label}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5nxrEoAHAUX"
      },
      "source": [
        "## PART - 3\n",
        "\n",
        "### FMNIST CNN Implimentation with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ccRJi8VXH3_O"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k41uN-aAIH6Y"
      },
      "outputs": [],
      "source": [
        "# Mapping the labels for the MNIST dataset\n",
        "labels_map = {\n",
        "    0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\",\n",
        "    5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_MUVyZ5Iksr",
        "outputId": "fee20814-4805-46b6-9c2a-cea97192b605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AEoqWEFz5Ms-"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_tkHuwEK7B",
        "outputId": "6914193e-b027-4100-ce9b-c27788c30802"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='sigmoid', input_shape=(28, 28, 1)),  # 16 filters (reduced), 3x3 kernel\n",
        "    MaxPooling2D(pool_size=(5, 5)),  # Max pooling with 2x2 pool size\n",
        "    # Flatten the output before passing to Dense layers\n",
        "    Flatten(),\n",
        "    Dense(64, activation='softmax'),  # Reduced from 128 to 64 units\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 units for classification\n",
        "])\n",
        "\n",
        "## change the architecture with CONV2D, Pooling, and Dense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nPfHtKytJd9Q"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "batch_size = 48\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlPkc9auJkET",
        "outputId": "ed426026-a519-43d4-a811-039b74e7c7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.0981 - loss: 2.3028\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.0969 - loss: 2.3024\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0964 - loss: 2.3020\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1077 - loss: 2.3015\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0918 - loss: 2.3002\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0962 - loss: 2.2981\n",
            "Test accuracy: 0.10000000149011612\n"
          ]
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig5GXhcG5fy6",
        "outputId": "b41b841c-b2e6-4520-e664-518039d33b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.0951 - loss: 2.2976 - val_accuracy: 0.0782 - val_loss: 2.2944\n",
            "Epoch 2/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1121 - loss: 2.2928 - val_accuracy: 0.1810 - val_loss: 2.2860\n",
            "Epoch 3/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.1941 - loss: 2.2815 - val_accuracy: 0.1964 - val_loss: 2.2593\n",
            "Epoch 4/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2090 - loss: 2.2439 - val_accuracy: 0.2071 - val_loss: 2.1862\n",
            "Epoch 5/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2047 - loss: 2.1652 - val_accuracy: 0.2084 - val_loss: 2.1000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2142 - loss: 2.0979\n",
            "Test accuracy: 0.2093999981880188\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "zJa4Lf76KZDM",
        "outputId": "5f20b1c1-f05c-40d1-f0d1-930d6501c654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_index = 27\n",
        "\n",
        "# Extract the test image and label\n",
        "test_image = x_test[image_index]\n",
        "test_label = np.argmax(y_test[image_index])\n",
        "\n",
        "# Reshape the test image for prediction (Keras expects a batch dimension)\n",
        "test_image_reshaped = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Make predictions on the test image\n",
        "predicted_label = np.argmax(model.predict(test_image_reshaped), axis=-1)\n",
        "\n",
        "# Plot the test image with predicted and actual labels\n",
        "plt.imshow(test_image, cmap='gray')\n",
        "plt.title(f'Predicted Label: {predicted_label[0]}, Actual Label: {test_label}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWO9KNmgQ0VX"
      },
      "source": [
        "### Just to explore TensorFlow Implemenation of CNN.\n",
        "\n",
        "Not Required For Submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8F7Lo_AOII",
        "outputId": "107054e4-528a-48a6-8b79-67bd294c8377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1010 - loss: 2.3026\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.0984 - loss: 2.3026\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1009 - loss: 2.3026\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1026 - loss: 2.3025\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1026 - loss: 2.3024\n",
            "313/313 - 2s - 7ms/step - accuracy: 0.1000 - loss: 2.3023\n",
            "\n",
            "Test accuracy: 0.10000000149011612\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (5, 5), activation='sigmoid', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='tanh'),\n",
        "    layers.MaxPooling2D((3, 3)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='softmax'),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='SGD',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images.reshape(-1, 28, 28, 1), train_labels, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images.reshape(-1, 28, 28, 1), test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK1xXT5W7cMS"
      },
      "source": [
        "## AUTOMATED TUNING (EXETENDED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIeVei_C8sVB"
      },
      "outputs": [],
      "source": [
        "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
